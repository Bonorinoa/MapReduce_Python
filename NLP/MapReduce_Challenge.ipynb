{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapReduce_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPIiRYHePmxs"
      },
      "outputs": [],
      "source": [
        "!rm -f hadoop-3.3.2.tar.gz*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Hdt7fQPtbh",
        "outputId": "882ece28-0512-437f-9f68-192d0bbce68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-25 23:26:36--  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638660563 (609M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.2.tar.gz’\n",
            "\n",
            "hadoop-3.3.2.tar.gz 100%[===================>] 609.07M  1.75MB/s    in 6m 36s  \n",
            "\n",
            "2022-04-25 23:33:13 (1.54 MB/s) - ‘hadoop-3.3.2.tar.gz’ saved [638660563/638660563]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.2.tar.gz"
      ],
      "metadata": {
        "id": "Mjg9gVsATdEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copy  hadoop file to user/local\n",
        "!cp -r hadoop-3.3.2/ /usr/local/"
      ],
      "metadata": {
        "id": "mBQe-eZupWZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiPPQ0EyPxF2",
        "outputId": "4b72b2b0-1ee2-4428-8e2b-22c4d9ff5dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To set the java path, go to /usr/local/hadoop-3.3.2/etc/hadoop/hadoop-env.sh then\n",
        "\n",
        "* Look for the line `export JAVA_HOME=`\n",
        "* Replace it with `export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/`\n",
        "* Remove the comments (the notebook will automatically save the shell script)"
      ],
      "metadata": {
        "id": "UrpjVBfYPy60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chek if Hadoop was correctly installed\n",
        "!/usr/local/hadoop-3.3.2/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaiKcOVNPw_k",
        "outputId": "dbdf4466-878c-4bf0-9474-60a6e726300d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2DCFF3xPw9N",
        "outputId": "403ada00-1228-41af-941a-43965c2438c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from mrjob) (3.13)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The goal is to compute the statistical support for the input list of tuples.\n",
        "### Problem:\n",
        "\n",
        "Let I = {i1, i2, ..., in} be a set of n items.\n",
        "Let D = {t1, t2, ..., tm} be a set of m transactions.\n",
        "\n",
        "We must identify an association of the form Y → X, where X, Y ⊆ I, and X ∩ Y = 0."
      ],
      "metadata": {
        "id": "1SyDLiUVQBke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Groceries.csv or tinyGroceries.csv\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0jLZ9QARHHUu",
        "outputId": "cf5c64eb-8363-438c-f8e7-1f5cdf09fb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4289df4-a34f-4c45-81ee-d90e8c6268cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b4289df4-a34f-4c45-81ee-d90e8c6268cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Groceries.csv to Groceries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file supportandsort.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import re\n",
        "\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "\n",
        "class supportandsort(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper,\n",
        "                   reducer=self.count_pairs),\n",
        "            MRStep(mapper=self.make_counts_key,\n",
        "                   reducer = self.output_support)\n",
        "        ]\n",
        "\n",
        "    def mapper(self, _, row):\n",
        "        \n",
        "        items = row.split(',')\n",
        "\n",
        "        combs = list(combinations(items, 2))\n",
        "\n",
        "        for pair in combs:\n",
        "\n",
        "          yield ( (pair, 1) )\n",
        "\n",
        "    def count_pairs(self, pair, counts):\n",
        "        yield  pair, sum(counts)\n",
        "\n",
        "    def make_counts_key(self, pair, count):\n",
        "        yield str(count).rjust(5,'0'), pair\n",
        "\n",
        "    def output_support(self, count, pairs):\n",
        "        for pair in pairs:\n",
        "            yield count, pair\n",
        "\n",
        "    # def reducer(self, key, value):\n",
        "    #     yield ( key, sum(value) )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    supportandsort.run() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0bqS49jPwkx",
        "outputId": "25a1ef80-cbf8-4ab3-8638-9c01fd362c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting supportandsort.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, let's get the logic of the code to count pairs of objects right with a smaller example"
      ],
      "metadata": {
        "id": "dcbyuT2_QYsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change below the name of the Python/MrJob program and the file(s) used by it\n",
        "# The datafiles can have extensions (e.g. .csv, .txt)  or be without extension.\n",
        "!python supportandsort.py Groceries.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2nlc6AvQf6E",
        "outputId": "026fe788-886c-450f-e390-9f3a1cc653f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/supportandsort.root.20220425.233453.253039\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/supportandsort.root.20220425.233453.253039/output\n",
            "Streaming final output from /tmp/supportandsort.root.20220425.233453.253039/output...\n",
            "\"00036\"\t[\"Ready made\", \"Fresh Vegetables\"]\n",
            "\"00036\"\t[\"Alcohol\", \"Fresh Vegetables\"]\n",
            "\"00040\"\t[\"Frozen foods\", \"Fresh Vegetables\"]\n",
            "\"00040\"\t[\"Toiletries\", \"Tinned Goods\"]\n",
            "\"00043\"\t[\"Fresh Vegetables\", \"Snacks\"]\n",
            "\"00046\"\t[\"Fresh Vegetables\", \"Bakery goods\"]\n",
            "\"00046\"\t[\"Ready made\", \"Toiletries\"]\n",
            "\"00047\"\t[\"Alcohol\", \"Toiletries\"]\n",
            "\"00050\"\t[\"Frozen foods\", \"Toiletries\"]\n",
            "\"00054\"\t[\"Fresh Vegetables\", \"Tinned Goods\"]\n",
            "\"00056\"\t[\"Bakery goods\", \"Toiletries\"]\n",
            "\"00058\"\t[\"Toiletries\", \"Snacks\"]\n",
            "\"00085\"\t[\"Frozen foods\", \"Milk\"]\n",
            "\"00090\"\t[\"Alcohol\", \"Milk\"]\n",
            "\"00098\"\t[\"Milk\", \"Snacks\"]\n",
            "\"00100\"\t[\"Milk\", \"Tinned Goods\"]\n",
            "\"00105\"\t[\"Ready made\", \"Milk\"]\n",
            "\"00110\"\t[\"Milk\", \"Bakery goods\"]\n",
            "\"00136\"\t[\"Alcohol\", \"Tinned Goods\"]\n",
            "\"00163\"\t[\"Frozen foods\", \"Tinned Goods\"]\n",
            "\"00166\"\t[\"Ready made\", \"Frozen foods\"]\n",
            "\"00167\"\t[\"Ready made\", \"Alcohol\"]\n",
            "\"00168\"\t[\"Frozen foods\", \"Snacks\"]\n",
            "\"00169\"\t[\"Alcohol\", \"Bakery goods\"]\n",
            "\"00005\"\t[\"Fresh meat\", \"Toiletries\"]\n",
            "\"00007\"\t[\"Fresh Vegetables\", \"Fresh meat\"]\n",
            "\"00013\"\t[\"Fresh Vegetables\", \"Toiletries\"]\n",
            "\"00014\"\t[\"Milk\", \"Fresh meat\"]\n",
            "\"00015\"\t[\"Bakery goods\", \"Fresh meat\"]\n",
            "\"00016\"\t[\"Alcohol\", \"Fresh meat\"]\n",
            "\"00019\"\t[\"Fresh meat\", \"Snacks\"]\n",
            "\"00019\"\t[\"Fresh meat\", \"Tinned Goods\"]\n",
            "\"00019\"\t[\"Frozen foods\", \"Fresh meat\"]\n",
            "\"00021\"\t[\"Ready made\", \"Fresh meat\"]\n",
            "\"00030\"\t[\"Milk\", \"Toiletries\"]\n",
            "\"00032\"\t[\"Fresh Vegetables\", \"Milk\"]\n",
            "\"00170\"\t[\"Ready made\", \"Tinned Goods\"]\n",
            "\"00172\"\t[\"Alcohol\", \"Snacks\"]\n",
            "\"00174\"\t[\"Frozen foods\", \"Bakery goods\"]\n",
            "\"00176\"\t[\"Snacks\", \"Tinned Goods\"]\n",
            "\"00179\"\t[\"Bakery goods\", \"Tinned Goods\"]\n",
            "\"00181\"\t[\"Frozen foods\", \"Alcohol\"]\n",
            "\"00183\"\t[\"Bakery goods\", \"Snacks\"]\n",
            "\"00192\"\t[\"Ready made\", \"Snacks\"]\n",
            "\"00201\"\t[\"Ready made\", \"Bakery goods\"]\n",
            "Removing temp directory /tmp/supportandsort.root.20220425.233453.253039...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's run it in Hadoop now"
      ],
      "metadata": {
        "id": "eHFMXhdGQmDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file ~/.mrjob.conf\n",
        "runners:\n",
        "    hadoop:\n",
        "      hadoop_bin: /usr/local/hadoop-3.3.2/bin/hadoop\n",
        "      hadoop_streaming_jar: /usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar\n",
        "      hadoop_tmp_dir: file:///content/tmp/mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy9fsfN2Ql4T",
        "outputId": "5dd603f1-ed64-4984-b542-df9ea2c91160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /root/.mrjob.conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change below the name of the Python/MrJob program and the file(s) used by it\n",
        "# Note that the option -r hadoop changes the execution  from local to the hadoop framework\n",
        "!python supportandsort.py -r hadoop  Groceries.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQrQKpWAQqDX",
        "outputId": "d013eaad-da58-4168-d4be-42be5f76ac2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using configs in /root/.mrjob.conf\n",
            "Using Hadoop version 3.3.2\n",
            "Creating temp directory /tmp/supportandsort.root.20220425.233509.579054\n",
            "uploading working dir files to file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd...\n",
            "Copying other local files to file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/\n",
            "Running step 1 of 2...\n",
            "  Loaded properties from hadoop-metrics2.properties\n",
            "  Scheduled Metric snapshot period at 10 second(s).\n",
            "  JobTracker metrics system started\n",
            "  JobTracker metrics system already initialized!\n",
            "  Total input files to process : 1\n",
            "  number of splits:1\n",
            "  Submitting tokens for job: job_local1548679558_0001\n",
            "  Executing with tokens: []\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local1548679558_0001_1385ceb9-2f4e-4c22-a292-5d406c2dba14/mrjob.zip <- /content/mrjob.zip\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/mrjob.zip as file:/tmp/hadoop-root/mapred/local/job_local1548679558_0001_1385ceb9-2f4e-4c22-a292-5d406c2dba14/mrjob.zip\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local1548679558_0001_e88ee27b-4ae7-4861-a87c-4a4ca3d5a0a1/setup-wrapper.sh <- /content/setup-wrapper.sh\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/setup-wrapper.sh as file:/tmp/hadoop-root/mapred/local/job_local1548679558_0001_e88ee27b-4ae7-4861-a87c-4a4ca3d5a0a1/setup-wrapper.sh\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/supportandsort.py as file:/tmp/hadoop-root/mapred/local/job_local1548679558_0001_57859cdb-f007-4d39-a4a8-b14239b10660/supportandsort.py\n",
            "  The url to track the job: http://localhost:8080/\n",
            "  Running job: job_local1548679558_0001\n",
            "  OutputCommitter set in config null\n",
            "  OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "  Waiting for map tasks\n",
            "  Starting task: attempt_local1548679558_0001_m_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Processing split: file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/Groceries.csv:0+26233\n",
            "  numReduceTasks: 1\n",
            "  (EQUATOR) 0 kvi 26214396(104857584)\n",
            "  mapreduce.task.io.sort.mb: 100\n",
            "  soft limit at 83886080\n",
            "  bufstart = 0; bufvoid = 104857600\n",
            "  kvstart = 26214396; length = 6553600\n",
            "  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, supportandsort.py, --step-num=0, --mapper]\n",
            "  mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "  mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "  map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "  map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "  mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "  map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "  mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "  user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "+ __mrjob_PWD=/content\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 supportandsort.py --step-num=0 --mapper\n",
            "  Job job_local1548679558_0001 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "  Records R/W=786/1\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  \n",
            "  Starting flush of map output\n",
            "  Spilling map output\n",
            "  bufstart = 0; bufend = 119169; bufvoid = 104857600\n",
            "  kvstart = 26214396(104857584); kvend = 26198612(104794448); length = 15785/6553600\n",
            "  Finished spill 0\n",
            "  Task:attempt_local1548679558_0001_m_000000_0 is done. And is in the process of committing\n",
            "  Records R/W=786/1\n",
            "  Task 'attempt_local1548679558_0001_m_000000_0' done.\n",
            "  Final Counters for attempt_local1548679558_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=603048\n",
            "\t\tFILE: Number of bytes written=1348190\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=786\n",
            "\t\tMap output records=3947\n",
            "\t\tMap output bytes=119169\n",
            "\t\tMap output materialized bytes=127069\n",
            "\t\tInput split bytes=138\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3947\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=26453\n",
            "  Finishing task: attempt_local1548679558_0001_m_000000_0\n",
            "  map task executor complete.\n",
            "  Waiting for reduce tasks\n",
            "  Starting task: attempt_local1548679558_0001_r_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@319a69c0\n",
            "  JobTracker metrics system already initialized!\n",
            "  MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "  attempt_local1548679558_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "  localfetcher#1 about to shuffle output of map attempt_local1548679558_0001_m_000000_0 decomp: 127065 len: 127069 to MEMORY\n",
            "  Read 127065 bytes from map-output for attempt_local1548679558_0001_m_000000_0\n",
            "  closeInMemoryFile -> map-output of size: 127065, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->127065\n",
            "  EventFetcher is interrupted.. Returning\n",
            "  1 / 1 copied.\n",
            "  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 127035 bytes\n",
            "  Merged 1 segments, 127065 bytes to disk to satisfy reduce memory limit\n",
            "  Merging 1 files, 127069 bytes from disk\n",
            "  Merging 0 segments, 0 bytes from memory into reduce\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 127035 bytes\n",
            "  1 / 1 copied.\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, supportandsort.py, --step-num=0, --reducer]\n",
            "  mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "  mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "+ __mrjob_PWD=/content\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "  R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 supportandsort.py --step-num=0 --reducer\n",
            "   map 100% reduce 0%\n",
            "  Records R/W=3947/1\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  Task:attempt_local1548679558_0001_r_000000_0 is done. And is in the process of committing\n",
            "  1 / 1 copied.\n",
            "  Task attempt_local1548679558_0001_r_000000_0 is allowed to commit now\n",
            "  Saved output of task 'attempt_local1548679558_0001_r_000000_0' to file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/step-output/0000\n",
            "  Records R/W=3947/1 > reduce\n",
            "  Task 'attempt_local1548679558_0001_r_000000_0' done.\n",
            "  Final Counters for attempt_local1548679558_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=857218\n",
            "\t\tFILE: Number of bytes written=1476726\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=45\n",
            "\t\tReduce shuffle bytes=127069\n",
            "\t\tReduce input records=3947\n",
            "\t\tReduce output records=45\n",
            "\t\tSpilled Records=3947\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=371195904\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1467\n",
            "  Finishing task: attempt_local1548679558_0001_r_000000_0\n",
            "  reduce task executor complete.\n",
            "   map 100% reduce 100%\n",
            "  Job job_local1548679558_0001 completed successfully\n",
            "  Output directory: file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/step-output/0000\n",
            "Counters: 30\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=26453\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1467\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1460266\n",
            "\t\tFILE: Number of bytes written=2824916\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tInput split bytes=138\n",
            "\t\tMap input records=786\n",
            "\t\tMap output bytes=119169\n",
            "\t\tMap output materialized bytes=127069\n",
            "\t\tMap output records=3947\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tReduce input groups=45\n",
            "\t\tReduce input records=3947\n",
            "\t\tReduce output records=45\n",
            "\t\tReduce shuffle bytes=127069\n",
            "\t\tShuffled Maps =1\n",
            "\t\tSpilled Records=7894\n",
            "\t\tTotal committed heap usage (bytes)=742391808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "Running step 2 of 2...\n",
            "  Loaded properties from hadoop-metrics2.properties\n",
            "  Scheduled Metric snapshot period at 10 second(s).\n",
            "  JobTracker metrics system started\n",
            "  JobTracker metrics system already initialized!\n",
            "  Total input files to process : 1\n",
            "  number of splits:1\n",
            "  Submitting tokens for job: job_local385929274_0001\n",
            "  Executing with tokens: []\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local385929274_0001_d10e7ac4-e1e5-4b8a-a24a-e10f4da7a481/mrjob.zip <- /content/mrjob.zip\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/mrjob.zip as file:/tmp/hadoop-root/mapred/local/job_local385929274_0001_d10e7ac4-e1e5-4b8a-a24a-e10f4da7a481/mrjob.zip\n",
            "  Creating symlink: /tmp/hadoop-root/mapred/local/job_local385929274_0001_84b27942-fc82-47da-9f89-1c6f9f0d9433/setup-wrapper.sh <- /content/setup-wrapper.sh\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/setup-wrapper.sh as file:/tmp/hadoop-root/mapred/local/job_local385929274_0001_84b27942-fc82-47da-9f89-1c6f9f0d9433/setup-wrapper.sh\n",
            "  Localized file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/files/wd/supportandsort.py as file:/tmp/hadoop-root/mapred/local/job_local385929274_0001_f3d110f1-4f77-44c6-a2d0-e0ad7be168a1/supportandsort.py\n",
            "  The url to track the job: http://localhost:8080/\n",
            "  Running job: job_local385929274_0001\n",
            "  OutputCommitter set in config null\n",
            "  OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "  Waiting for map tasks\n",
            "  Starting task: attempt_local385929274_0001_m_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Processing split: file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/step-output/0000/part-00000:0+1447\n",
            "  numReduceTasks: 1\n",
            "  (EQUATOR) 0 kvi 26214396(104857584)\n",
            "  mapreduce.task.io.sort.mb: 100\n",
            "  soft limit at 83886080\n",
            "  bufstart = 0; bufvoid = 104857600\n",
            "  kvstart = 26214396; length = 6553600\n",
            "  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, supportandsort.py, --step-num=1, --mapper]\n",
            "  mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "  mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "  map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "  map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "  mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "  mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "  map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "  mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "  mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "  mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "  user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ __mrjob_PWD=/content\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 supportandsort.py --step-num=1 --mapper\n",
            "  Job job_local385929274_0001 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "  Records R/W=45/1\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  \n",
            "  Starting flush of map output\n",
            "  Spilling map output\n",
            "  bufstart = 0; bufend = 1656; bufvoid = 104857600\n",
            "  kvstart = 26214396(104857584); kvend = 26214220(104856880); length = 177/6553600\n",
            "  Finished spill 0\n",
            "  Task:attempt_local385929274_0001_m_000000_0 is done. And is in the process of committing\n",
            "  Records R/W=45/1\n",
            "  Task 'attempt_local385929274_0001_m_000000_0' done.\n",
            "  Final Counters for attempt_local385929274_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=578074\n",
            "\t\tFILE: Number of bytes written=1219752\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=45\n",
            "\t\tMap output records=45\n",
            "\t\tMap output bytes=1656\n",
            "\t\tMap output materialized bytes=1752\n",
            "\t\tInput split bytes=146\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=45\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1471\n",
            "  Finishing task: attempt_local385929274_0001_m_000000_0\n",
            "  map task executor complete.\n",
            "  Waiting for reduce tasks\n",
            "  Starting task: attempt_local385929274_0001_r_000000_0\n",
            "  File Output Committer Algorithm version is 2\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "   Using ResourceCalculatorProcessTree : [ ]\n",
            "  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d8d68c6\n",
            "  JobTracker metrics system already initialized!\n",
            "  MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "  attempt_local385929274_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "  localfetcher#1 about to shuffle output of map attempt_local385929274_0001_m_000000_0 decomp: 1748 len: 1752 to MEMORY\n",
            "  Read 1748 bytes from map-output for attempt_local385929274_0001_m_000000_0\n",
            "  closeInMemoryFile -> map-output of size: 1748, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1748\n",
            "  EventFetcher is interrupted.. Returning\n",
            "  1 / 1 copied.\n",
            "  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 1738 bytes\n",
            "  Merged 1 segments, 1748 bytes to disk to satisfy reduce memory limit\n",
            "  Merging 1 files, 1752 bytes from disk\n",
            "  Merging 0 segments, 0 bytes from memory into reduce\n",
            "  Merging 1 sorted segments\n",
            "  Down to the last merge-pass, with 1 segments left of total size: 1738 bytes\n",
            "  1 / 1 copied.\n",
            "  PipeMapRed exec [/bin/sh, -ex, setup-wrapper.sh, python3, supportandsort.py, --step-num=1, --reducer]\n",
            "  mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "  mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "  R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ __mrjob_PWD=/content\n",
            "+ exec\n",
            "+ python3 -c import fcntl; fcntl.flock(9, fcntl.LOCK_EX)\n",
            "  R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "+ export PYTHONPATH=/content/mrjob.zip:/env/python\n",
            "+ exec\n",
            "+ cd /content\n",
            "+ python3 supportandsort.py --step-num=1 --reducer\n",
            "  Records R/W=45/1\n",
            "   map 100% reduce 0%\n",
            "  MRErrorThread done\n",
            "  mapRedFinished\n",
            "  Task:attempt_local385929274_0001_r_000000_0 is done. And is in the process of committing\n",
            "  1 / 1 copied.\n",
            "  Task attempt_local385929274_0001_r_000000_0 is allowed to commit now\n",
            "  Saved output of task 'attempt_local385929274_0001_r_000000_0' to file:/content/tmp/mrjob/supportandsort.root.20220425.233509.579054/output\n",
            "  Records R/W=45/1 > reduce\n",
            "  Task 'attempt_local385929274_0001_r_000000_0' done.\n",
            "  Final Counters for attempt_local385929274_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=581610\n",
            "\t\tFILE: Number of bytes written=1223184\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=40\n",
            "\t\tReduce shuffle bytes=1752\n",
            "\t\tReduce input records=45\n",
            "\t\tReduce output records=45\n",
            "\t\tSpilled Records=45\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1680\n",
            "  Finishing task: attempt_local385929274_0001_r_000000_0\n",
            "  reduce task executor complete.\n",
            "   map 100% reduce 100%\n",
            "  Job job_local385929274_0001 completed successfully\n",
            "  Output directory: file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/output\n",
            "Counters: 30\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1471\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1680\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1159684\n",
            "\t\tFILE: Number of bytes written=2442936\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tInput split bytes=146\n",
            "\t\tMap input records=45\n",
            "\t\tMap output bytes=1656\n",
            "\t\tMap output materialized bytes=1752\n",
            "\t\tMap output records=45\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tReduce input groups=40\n",
            "\t\tReduce input records=45\n",
            "\t\tReduce output records=45\n",
            "\t\tReduce shuffle bytes=1752\n",
            "\t\tShuffled Maps =1\n",
            "\t\tSpilled Records=90\n",
            "\t\tTotal committed heap usage (bytes)=687865856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "job output is in file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/output\n",
            "Streaming final output from file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054/output...\n",
            "\"00005\"\t[\"Fresh meat\", \"Toiletries\"]\n",
            "\"00007\"\t[\"Fresh Vegetables\", \"Fresh meat\"]\n",
            "\"00013\"\t[\"Fresh Vegetables\", \"Toiletries\"]\n",
            "\"00014\"\t[\"Milk\", \"Fresh meat\"]\n",
            "\"00015\"\t[\"Bakery goods\", \"Fresh meat\"]\n",
            "\"00016\"\t[\"Alcohol\", \"Fresh meat\"]\n",
            "\"00019\"\t[\"Fresh meat\", \"Snacks\"]\n",
            "\"00019\"\t[\"Fresh meat\", \"Tinned Goods\"]\n",
            "\"00019\"\t[\"Frozen foods\", \"Fresh meat\"]\n",
            "\"00021\"\t[\"Ready made\", \"Fresh meat\"]\n",
            "\"00030\"\t[\"Milk\", \"Toiletries\"]\n",
            "\"00032\"\t[\"Fresh Vegetables\", \"Milk\"]\n",
            "\"00036\"\t[\"Ready made\", \"Fresh Vegetables\"]\n",
            "\"00036\"\t[\"Alcohol\", \"Fresh Vegetables\"]\n",
            "\"00040\"\t[\"Toiletries\", \"Tinned Goods\"]\n",
            "\"00040\"\t[\"Frozen foods\", \"Fresh Vegetables\"]\n",
            "\"00043\"\t[\"Fresh Vegetables\", \"Snacks\"]\n",
            "\"00046\"\t[\"Ready made\", \"Toiletries\"]\n",
            "\"00046\"\t[\"Fresh Vegetables\", \"Bakery goods\"]\n",
            "\"00047\"\t[\"Alcohol\", \"Toiletries\"]\n",
            "\"00050\"\t[\"Frozen foods\", \"Toiletries\"]\n",
            "\"00054\"\t[\"Fresh Vegetables\", \"Tinned Goods\"]\n",
            "\"00056\"\t[\"Bakery goods\", \"Toiletries\"]\n",
            "\"00058\"\t[\"Toiletries\", \"Snacks\"]\n",
            "\"00085\"\t[\"Frozen foods\", \"Milk\"]\n",
            "\"00090\"\t[\"Alcohol\", \"Milk\"]\n",
            "\"00098\"\t[\"Milk\", \"Snacks\"]\n",
            "\"00100\"\t[\"Milk\", \"Tinned Goods\"]\n",
            "\"00105\"\t[\"Ready made\", \"Milk\"]\n",
            "\"00110\"\t[\"Milk\", \"Bakery goods\"]\n",
            "\"00136\"\t[\"Alcohol\", \"Tinned Goods\"]\n",
            "\"00163\"\t[\"Frozen foods\", \"Tinned Goods\"]\n",
            "\"00166\"\t[\"Ready made\", \"Frozen foods\"]\n",
            "\"00167\"\t[\"Ready made\", \"Alcohol\"]\n",
            "\"00168\"\t[\"Frozen foods\", \"Snacks\"]\n",
            "\"00169\"\t[\"Alcohol\", \"Bakery goods\"]\n",
            "\"00170\"\t[\"Ready made\", \"Tinned Goods\"]\n",
            "\"00172\"\t[\"Alcohol\", \"Snacks\"]\n",
            "\"00174\"\t[\"Frozen foods\", \"Bakery goods\"]\n",
            "\"00176\"\t[\"Snacks\", \"Tinned Goods\"]\n",
            "\"00179\"\t[\"Bakery goods\", \"Tinned Goods\"]\n",
            "\"00181\"\t[\"Frozen foods\", \"Alcohol\"]\n",
            "\"00183\"\t[\"Bakery goods\", \"Snacks\"]\n",
            "\"00192\"\t[\"Ready made\", \"Snacks\"]\n",
            "\"00201\"\t[\"Ready made\", \"Bakery goods\"]\n",
            "Removing HDFS temp directory file:///content/tmp/mrjob/supportandsort.root.20220425.233509.579054...\n",
            "Removing temp directory /tmp/supportandsort.root.20220425.233509.579054...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W12FRIvwontp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}